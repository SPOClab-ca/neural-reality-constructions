{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonse Sentence Generation\n",
    "\n",
    "Generate nonsense sentences similar to Johnson and Goldberg (2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import scipy\n",
    "\n",
    "import src.sent_encoder\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain list of singular nouns, adjectives, past-tense verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "penn = nltk.corpus.reader.bracket_parse.BracketParseCorpusReader(\"../data/PTB3\", \".*\\.tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49207"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(penn.parsed_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_nouns = defaultdict(int)\n",
    "adjectives = defaultdict(int)\n",
    "past_verbs = defaultdict(int)\n",
    "\n",
    "for tree in penn.parsed_sents():\n",
    "  for leaf in tree.subtrees(lambda t: t.height() == 2):\n",
    "    if leaf.label() == \"NN\":\n",
    "      singular_nouns[leaf[0].lower()] += 1\n",
    "    if leaf.label() == \"JJ\":\n",
    "      adjectives[leaf[0].lower()] += 1\n",
    "    if leaf.label() == \"VBD\":\n",
    "      past_verbs[leaf[0].lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words that are too low-frequency\n",
    "singular_nouns = dict(filter(lambda w: w[1] >= 10, singular_nouns.items()))\n",
    "adjectives = dict(filter(lambda w: w[1] >= 10, adjectives.items()))\n",
    "past_verbs = dict(filter(lambda w: w[1] >= 10, past_verbs.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996\n",
      "843\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "singular_nouns = list(sorted(singular_nouns))\n",
    "adjectives = list(sorted(adjectives))\n",
    "past_verbs = list(sorted(past_verbs))\n",
    "\n",
    "print(len(singular_nouns))\n",
    "print(len(adjectives))\n",
    "print(len(past_verbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get contextual vecs for gave/made/put/took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/bnc.pkl\", \"rb\") as f:\n",
    "  bnc_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = 11\n",
    "enc = src.sent_encoder.SentEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_vecs = {\n",
    "  'gave': enc.avg_contextual_word_vec(bnc_data, \"gave\")[LAYER],\n",
    "  'made': enc.avg_contextual_word_vec(bnc_data, \"made\")[LAYER],\n",
    "  'put': enc.avg_contextual_word_vec(bnc_data, \"put\")[LAYER],\n",
    "  'took': enc.avg_contextual_word_vec(bnc_data, \"took\")[LAYER],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sentences of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(12345)\n",
    "NUM_SENTENCES_PER_CXN = 1000\n",
    "templated_sentences = defaultdict(list)\n",
    "\n",
    "# Ditransitive: S/he nonseV-ed him/her the nonseN\n",
    "for i in range(NUM_SENTENCES_PER_CXN):\n",
    "  pronoun1 = random.choice([\"He\", \"She\"])\n",
    "  pronoun2 = random.choice([\"him\", \"her\"])\n",
    "  nonse_verb = random.choice(past_verbs)\n",
    "  nonse_noun = random.choice(singular_nouns)\n",
    "  templated_sentences['ditransitive'].append((\n",
    "    f\"{pronoun1} {nonse_verb} {pronoun2} the {nonse_noun}.\",\n",
    "    nonse_verb\n",
    "  ))\n",
    "  \n",
    "# Resultative: S/he nonseV-ed it nonseAdj.\n",
    "for i in range(NUM_SENTENCES_PER_CXN):\n",
    "  pronoun1 = random.choice([\"He\", \"She\"])\n",
    "  nonse_verb = random.choice(past_verbs)\n",
    "  nonse_adj = random.choice(adjectives)\n",
    "  templated_sentences['resultative'].append((\n",
    "    f\"{pronoun1} {nonse_verb} it {nonse_adj}.\",\n",
    "    nonse_verb\n",
    "  ))\n",
    "  \n",
    "# Caused-motion: S/he nonseV-ed it on the nonseN.\n",
    "for i in range(NUM_SENTENCES_PER_CXN):\n",
    "  pronoun1 = random.choice([\"He\", \"She\"])\n",
    "  nonse_verb = random.choice(past_verbs)\n",
    "  nonse_noun = random.choice(singular_nouns)\n",
    "  templated_sentences['caused-motion'].append((\n",
    "    f\"{pronoun1} {nonse_verb} it on the {nonse_noun}.\",\n",
    "    nonse_verb\n",
    "  ))\n",
    "  \n",
    "# Removal: S/he nonseV-ed it from him/her.\n",
    "for i in range(NUM_SENTENCES_PER_CXN):\n",
    "  pronoun1 = random.choice([\"He\", \"She\"])\n",
    "  pronoun2 = random.choice([\"him\", \"her\"])\n",
    "  nonse_verb = random.choice(past_verbs)\n",
    "  templated_sentences['removal'].append((\n",
    "    f\"{pronoun1} {nonse_verb} it from {pronoun2}.\",\n",
    "    nonse_verb\n",
    "  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get distances from cxn-verbs to proto-verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_dist_results = []\n",
    "\n",
    "for cxn_type, cxn_sentences_and_verbs in templated_sentences.items():\n",
    "  cxn_sentences = [t[0] for t in cxn_sentences_and_verbs]\n",
    "  cxn_verbs = [t[1] for t in cxn_sentences_and_verbs]\n",
    "  cxn_verb_vecs = enc.sentence_vecs(cxn_sentences, cxn_verbs)[:, LAYER]\n",
    "  \n",
    "  for proto_verb, proto_verb_vec in prototype_vecs.items():\n",
    "    for i, cxn_verb_vec in enumerate(cxn_verb_vecs):\n",
    "      dist = np.linalg.norm(proto_verb_vec - cxn_verb_vec)\n",
    "      #dist = scipy.spatial.distance.cosine(proto_verb_vec, cxn_verb_vec)\n",
    "      verb_dist_results.append(pd.Series({\n",
    "        'cxn_sentence': cxn_sentences[i],\n",
    "        'cxn': cxn_type,\n",
    "        'verb': proto_verb,\n",
    "        'dist': dist,\n",
    "      }))\n",
    "      \n",
    "verb_dist_results = pd.DataFrame(verb_dist_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ditransitive gave 11.863252089500428 1.206948526117271\n",
      "resultative gave 11.87262932062149 1.2547931321904577\n",
      "caused-motion gave 11.70208685874939 1.200947899060393\n",
      "removal gave 11.7545820646286 1.4879872473417624\n",
      "ditransitive made 12.23942378282547 1.2489061122647556\n",
      "resultative made 11.641261716842651 1.1785564985557508\n",
      "caused-motion made 11.631052313804627 1.0693310761089205\n",
      "removal made 11.979291493892669 1.4244105349458458\n",
      "ditransitive put 12.494220232963562 1.2612567807272912\n",
      "resultative put 11.82379406452179 1.323393038165233\n",
      "caused-motion put 11.424014540672303 1.0233379854745843\n",
      "removal put 11.953893012046814 1.3660698962034628\n",
      "ditransitive took 12.281264220714569 1.2706358089528065\n",
      "resultative took 11.813029750823974 1.2687060894271465\n",
      "caused-motion took 11.608704370975495 1.1981390586590446\n",
      "removal took 11.504822383403779 1.2625777348991174\n"
     ]
    }
   ],
   "source": [
    "for verb in prototype_vecs.keys():\n",
    "  for cxn in templated_sentences.keys():\n",
    "    m = verb_dist_results[(verb_dist_results.cxn == cxn) & (verb_dist_results.verb == verb)].mean()\n",
    "    sd = verb_dist_results[(verb_dist_results.cxn == cxn) & (verb_dist_results.verb == verb)].std()\n",
    "    print(cxn, verb, float(m), float(sd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
